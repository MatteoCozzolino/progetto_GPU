{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WoJbB3T5Vkw-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoJbB3T5Vkw-"
      },
      "source": [
        "# ▶️ CUDA setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fht2Wy8wVkxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d93dbd3-1dea-494f-bfbc-1ff68c4c90be"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jP2H_YJVkxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41973cd-9180-4bba-eca1-ccab0ae7d643"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 12 12:22:42 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cGSqZovVkxK"
      },
      "source": [
        "## NVCC Plugin for Jupyter notebook\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCVhMkqYVkxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322f5c84-5369-458b-a4d1-3e0c27798c0d"
      },
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-le98z6zk\n",
            "  Running command git clone -q https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-le98z6zk\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4306 sha256=13aa44e33023324ee505691530497147e4c8518030a7d8b0717349d941047ce9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4xcz8go5/wheels/ca/33/8d/3c86eb85e97d2b6169d95c6e8f2c297fdec60db6e84cb56f5e\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6PDOytTVkxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4677ef98-338c-4f1f-d316-c893d64f62f7"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bash and data setup"
      ],
      "metadata": {
        "id": "cReFlD-VRfZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Bash setup\n",
        "%%writefile /root/.bashrc\n",
        "\n",
        "# If not running interactively, don't do anything\n",
        "[ -z \"$PS1\" ] && return\n",
        "\n",
        "# don't put duplicate lines in the history. See bash(1) for more options\n",
        "# ... or force ignoredups and ignorespace\n",
        "HISTCONTROL=ignoredups:ignorespace\n",
        "\n",
        "# append to the history file, don't overwrite it\n",
        "shopt -s histappend\n",
        "\n",
        "# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)\n",
        "HISTSIZE=10000\n",
        "HISTFILESIZE=20000\n",
        "\n",
        "# check the window size after each command and, if necessary,\n",
        "# update the values of LINES and COLUMNS.\n",
        "shopt -s checkwinsize\n",
        "\n",
        "# make less more friendly for non-text input files, see lesspipe(1)\n",
        "[ -x /usr/bin/lesspipe ] && eval \"$(SHELL=/bin/sh lesspipe)\"\n",
        "\n",
        "PS1='\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ '\n",
        "\n",
        "# enable color support of ls and also add handy aliases\n",
        "if [ -x /usr/bin/dircolors ]; then\n",
        "    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\n",
        "    alias ls='ls --color=auto'\n",
        "    #alias dir='dir --color=auto'\n",
        "    #alias vdir='vdir --color=auto'\n",
        "\n",
        "    alias grep='grep --color=auto'\n",
        "    alias fgrep='fgrep --color=auto'\n",
        "    alias egrep='egrep --color=auto'\n",
        "fi\n",
        "\n",
        "# some more ls aliases\n",
        "alias ll='ls -lF'\n",
        "alias la='ls -A'\n",
        "alias l='ls -CF'\n",
        "\n",
        "# path setup\n",
        "export PATH=\"./:/usr/local/cuda/bin:$PATH\""
      ],
      "metadata": {
        "id": "O8ICSyy8_GEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "129e579c-7210-4180-d2e8-53575bf8813f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /root/.bashrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source /root/.bashrc"
      ],
      "metadata": {
        "id": "QxIfKO3Ghf7g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/giulianogrossi/GPUcomputing.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eqv8awNahJNN",
        "outputId": "55a3b23d-fe4d-4bb4-bb64-010cf7bcc547"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GPUcomputing'...\n",
            "remote: Enumerating objects: 219, done.\u001b[K\n",
            "remote: Counting objects: 100% (219/219), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 219 (delta 79), reused 189 (delta 49), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (219/219), 1.30 MiB | 16.21 MiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA\n"
      ],
      "metadata": {
        "id": "pkUgpvpQZmAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La tecnica PCA per l'estrazione di componenti pricipali da un dataset contenente una grande quantità di rumore avviene attraverso l'elaborazione di matrici per ottenere un cambiamento di base, ovvero un punto di vista, dei dati stessi. Una matrice che rappresenta un dataset da elaborare sarà costituita da righe corrispondenti al numero di tipi di misurazioni e da colonne corrispondenti al numero di misurazioni. Le strategie adottate sono due:\n",
        "\n",
        "- PCA attraverso il calcolo di autovalori e autovettori della matrice di covarianza; gli autovettori saranno i componenti principali\n",
        "\n",
        "- PCA attraverso il calcolo SVD da cui si ottiene una matrice ortonormale corrispondente agli autovettori della matrice di covarianza\n",
        "\n",
        "L'elaborazione è effettuata attraverso librerie CuBlas e CuSolver per il calcolo di prodotti matriciali, autovettori e SVD. \n",
        "Viene inoltre calcolato il tempo di esecuzione dei singoli passi dell'elaborazione e le due strategie vengono confrontate."
      ],
      "metadata": {
        "id": "ophmCqH1AywR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/pca.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#include \"../GPUcomputing/utils/common.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <cusparse.h>\n",
        "#include <cuda.h>\n",
        "#include \"cublas_v2.h\"\n",
        "#include <cusolverDn.h>\n",
        "\n",
        "#define BLOCKS  24\n",
        "#define R 2000   //righe corrispondenti al numero di tipi di misurazioni\n",
        "#define C 300    //colonne corrispondenti al numero di misurazioni\n",
        "#define P 5     //numero di Principal components a cui si è interessati\n",
        "\n",
        "void generateMatrix(int, int, float**);\n",
        "float* orderInversion(int, float*);\n",
        "double var(int, int, float*);\n",
        "__global__ void meanSubtraction(float*, float*, int, int);\n",
        "double serialPCA(float*);\n",
        "void transpose(float*, int, int, float*);\n",
        "void multiply(float*, int, int, float*, int, int, float*);\n",
        "float maxind(float*, int, int);\n",
        "void update(int, float, float*, bool*, int*);\n",
        "void rotate(int, int, int, int, float*, double, double);\n",
        "void compute_V(float*, float*, float**, float**);\n",
        "void initialize_identity(float*, int);\n",
        "void mergesort(float*, int, int*, int, int);\n",
        "void merge(float*, int*, int, int, int);\n",
        "\n",
        "int main (int argc, char *argv[]) {\n",
        "    \n",
        "    int height = R;\n",
        "    int width = C;\n",
        "    int size = height * width;\n",
        "    float alpha;\n",
        "    float beta = 0.0f;\n",
        "    float executionTime = 0.0f;\n",
        "\n",
        "    float *matrix, *covariance, *principal_components;\n",
        "    float *eigenvalues, *eigenvectors;\n",
        "    float *d_matrix, *d_C;\n",
        "    float *d_eigenval, *d_eigenvect, *d_work;\n",
        "    float *d_rwork = nullptr;\n",
        "\n",
        "    //Matrici ausiliarie per il calcolo di SVD\n",
        "    float *U, *S, *VT;\n",
        "    float *d_U, *d_S, *d_VT;\n",
        "\n",
        "    cudaStream_t stream;\n",
        "\n",
        "    cublasHandle_t handle;\n",
        "\n",
        "    cusolverDnHandle_t cusolverH;\n",
        "    int lwork = 0;\n",
        "    int *d_info;\n",
        "    int info = 0;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "\t  cudaEventCreate(&start);\n",
        "\t  cudaEventCreate(&stop);\n",
        "\n",
        "\n",
        "    //Generazione matrice di input, calcolo della media e sottrazione di essa da ciascun elemento dell'input\n",
        "\n",
        "    generateMatrix(height, width, &matrix);\n",
        "\n",
        "    CHECK(cudaMalloc((void **)&d_matrix, size * sizeof(float)));\n",
        "    CHECK(cudaMalloc((void **)&d_C, size * sizeof(float)));\n",
        "    CHECK(cudaMemcpy(d_matrix, matrix, size * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    dim3 block(BLOCKS, BLOCKS);\n",
        "\t  dim3 grid((width + block.x - 1) / block.x, (height + block.y - 1) / block.y);\n",
        "    meanSubtraction<<<grid, block>>>(d_matrix, d_C, width, height);\n",
        "    CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "\t  CHECK(cudaMemcpy(matrix, d_C, size * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    CHECK(cudaFree(d_matrix));\n",
        "    CHECK(cudaFree(d_C));\n",
        "  \n",
        "    //Calcolo del prodotto della matrice per la sua trasposta; Sgemm ovvero alpha * A * A_Trasposta + beta * C \n",
        "    //Che simula il calcolo di matrice di covarianza S = 1/n-1 * A * A_Trasposta\n",
        "\n",
        "    alpha = 1 / (float)(width - 1);\n",
        "\n",
        "    CHECK_CUBLAS(cublasCreate(&handle));\n",
        "    CHECK(cudaMalloc((void **)&d_matrix, size * sizeof(float)));\n",
        "    CHECK(cudaMalloc((void **)&d_C, height * height * sizeof(float)));\n",
        "    CHECK(cudaMemset(d_C, 0, height * height *sizeof(float)));\n",
        "    CHECK_CUBLAS(cublasSetMatrix(height, width, sizeof(float), matrix, height, d_matrix, height));\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    CHECK_CUBLAS(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_T, height, width, width, &alpha, d_matrix, height, d_matrix, height, &beta, d_C, height));\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    float milliseconds;\n",
        "\t  cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\t  printf(\" Matrix product for its transpose took: %.5f (sec)\\n\\n\", milliseconds / 1000.0);\n",
        "    executionTime += milliseconds;\n",
        "\n",
        "    covariance = (float *)malloc (height * height * sizeof(float));\n",
        "    CHECK_CUBLAS(cublasGetMatrix(height, height, sizeof(float), d_C, height, covariance, height));\n",
        "\n",
        "    CHECK(cudaFree(d_matrix));\n",
        "    CHECK(cudaFree(d_C));\n",
        "\n",
        "    //Calcolo Autovalori \n",
        "\n",
        "    eigenvalues = (float *)malloc (height * sizeof(float));\n",
        "    eigenvectors = (float *)malloc (height * height * sizeof(float));\n",
        "\n",
        "    cusolverDnCreate(&cusolverH);\n",
        "    cudaStreamCreate(&stream);\n",
        "    cusolverDnSetStream(cusolverH, stream);\n",
        "\n",
        "    CHECK(cudaMalloc((void **)&d_matrix, height * height * sizeof(float)));\n",
        "    CHECK(cudaMalloc((void **)&d_eigenval, height * sizeof(float)));\n",
        "    CHECK(cudaMalloc((void **)&d_info, sizeof(int)));\n",
        "\n",
        "    cusolverDnSsyevd_bufferSize(cusolverH, CUSOLVER_EIG_MODE_VECTOR, CUBLAS_FILL_MODE_LOWER, height, d_matrix, height, d_eigenval, &lwork);\n",
        "    CHECK(cudaMalloc((void **)&d_work, sizeof(float) * lwork));\n",
        "    CHECK(cudaMemcpyAsync(d_matrix, covariance, sizeof(float) * height * height, cudaMemcpyHostToDevice, stream));\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    cusolverDnSsyevd(cusolverH, CUSOLVER_EIG_MODE_VECTOR, CUBLAS_FILL_MODE_LOWER, height, d_matrix, height, d_eigenval, d_work, lwork, d_info);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\t  cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\t  printf(\" Eigenvalues and Eigenvectors calculation took: %.5f (sec)\\n\\n\", milliseconds / 1000.0);\n",
        "    executionTime += milliseconds;\n",
        "\n",
        "    CHECK(cudaMemcpyAsync(eigenvalues, d_eigenval, sizeof(float) * height, cudaMemcpyDeviceToHost, stream));\n",
        "    CHECK(cudaMemcpyAsync(eigenvectors, d_matrix, sizeof(float) * height * height, cudaMemcpyDeviceToHost, stream));\n",
        "    CHECK(cudaMemcpyAsync(&info, d_info, sizeof(int), cudaMemcpyDeviceToHost, stream));\n",
        "\n",
        "    CHECK(cudaStreamSynchronize(stream));\n",
        "\n",
        "    //Controllo la correttezza dell'operazione\n",
        "\n",
        "    if (info < 0) {\n",
        "            printf(\"%d-th parameter is wrong \\n\", info);\n",
        "            return 0;\n",
        "        }\n",
        "\n",
        "    CHECK(cudaFree(d_matrix));\n",
        "    CHECK(cudaFree(d_eigenval));\n",
        "    CHECK(cudaFree(d_info));\n",
        "    CHECK(cudaFree(d_work));\n",
        "\n",
        "    cusolverDnDestroy(cusolverH);\n",
        "    CHECK(cudaStreamDestroy(stream));\n",
        "\n",
        "    //Ordino gli Autovalori in ordire decrescente\n",
        "\n",
        "    eigenvalues = orderInversion(height, eigenvalues);\n",
        "\n",
        "    //Calcolo i Principal Components: T =  eigenvectors * matrix\n",
        "\n",
        "    principal_components = (float *)malloc (height * width * sizeof(float));\n",
        "\n",
        "    CHECK_CUBLAS(cublasCreate(&handle));\n",
        "    CHECK(cudaMalloc((void **)&d_matrix, size * sizeof(float)));\n",
        "    CHECK(cudaMalloc((void **)&d_eigenvect, height * height * sizeof(float)));\n",
        "    CHECK(cudaMalloc((void **)&d_C, width * height * sizeof(float)));\n",
        "    CHECK(cudaMemset(d_C, 0, width * height *sizeof(float)));\n",
        "    CHECK_CUBLAS(cublasSetMatrix(height, width, sizeof(float), matrix, height, d_matrix, height));\n",
        "    CHECK_CUBLAS(cublasSetMatrix(height, height, sizeof(float), eigenvectors, height, d_eigenvect, height));\n",
        "\n",
        "    alpha = 1.0f;\n",
        "    cudaEventRecord(start);\n",
        "    CHECK_CUBLAS(cublasSgemm(handle, CUBLAS_OP_T, CUBLAS_OP_N, height, width, height, &alpha, d_eigenvect, height, d_matrix, height, &beta, d_C, height));\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\t  cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\t  printf(\" Principal Components calculation took: %.5f (sec)\\n\\n\", milliseconds / 1000.0);\n",
        "    executionTime += milliseconds;\n",
        "\n",
        "    CHECK_CUBLAS(cublasGetMatrix(height, width, sizeof(float), d_C, height, principal_components, height));\n",
        "\n",
        "    CHECK(cudaFree(d_matrix));\n",
        "    CHECK(cudaFree(d_C));\n",
        "    CHECK(cudaFree(d_eigenvect));\n",
        "\n",
        "    //Valuto quanti Principal components tenere per la riduzione, il parametro r descrive quanta varianza si è interessati a tenere\n",
        "\n",
        "    int r = P;\n",
        "   \n",
        "    double variance = var(r, height, eigenvalues);\n",
        "\n",
        "    printf(\" With a parameter of %d elements we were able to keep %f percentage of principal components\\n\", r, variance);\n",
        "    printf(\" PCA technique through eigenvalues calculation took in total %f seconds\\n\\n\", executionTime / 1000.0);\n",
        "    float executionTimeSVD = 0.0f;\n",
        "\n",
        "    free(covariance);\n",
        "    free(eigenvalues);\n",
        "    free(eigenvectors);\n",
        "    free(principal_components);\n",
        "\n",
        "    //Calcolo ora la SVD\n",
        "\n",
        "    cusolverDnCreate(&cusolverH);\n",
        "    cudaStreamCreate(&stream);\n",
        "    cusolverDnSetStream(cusolverH, stream);\n",
        "\n",
        "    U = (float *)malloc (height *height * sizeof(float));\n",
        "    S = (float *)malloc (size * sizeof(float));\n",
        "    VT = (float *)malloc (size * sizeof(float));\n",
        "\n",
        "    CHECK(cudaMalloc((void **)&d_matrix, size * sizeof(float)));\n",
        "    CHECK(cudaMalloc((void **)&d_S,size * sizeof(float)));\n",
        "    CHECK(cudaMalloc((void **)&d_U, height *  height * sizeof(float)));\n",
        "    CHECK(cudaMalloc((void **)&d_VT, height * width * sizeof(float)));\n",
        "    CHECK(cudaMalloc((void **)&d_info, sizeof(int)));\n",
        "\n",
        "    cusolverDnSgesvd_bufferSize(cusolverH, height, width, &lwork);\n",
        "    CHECK(cudaMalloc((void **)&d_work, sizeof(float) * lwork));\n",
        "    CHECK(cudaMemcpyAsync(d_matrix, matrix, sizeof(float) * size, cudaMemcpyHostToDevice, stream));\n",
        "\n",
        "    signed char jobu = 'A'; \n",
        "    signed char jobvt = 'A';\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    cusolverDnSgesvd(cusolverH, jobu, jobvt, height, width, d_matrix, height, d_S, d_U, height, d_VT, height, d_work, lwork, d_rwork, d_info);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\t  cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\t  printf(\" SVD calculation took: %.5f (sec)\\n\", milliseconds / 1000.0);\n",
        "    executionTimeSVD += milliseconds;\n",
        "\n",
        "    CHECK(cudaMemcpyAsync(U, d_U, sizeof(float) * height * height, cudaMemcpyDeviceToHost, stream));\n",
        "    CHECK(cudaMemcpyAsync(VT, d_VT, sizeof(float) * size, cudaMemcpyDeviceToHost, stream));\n",
        "    CHECK(cudaMemcpyAsync(S, d_S, sizeof(float) * size, cudaMemcpyDeviceToHost, stream));\n",
        "    CHECK(cudaMemcpyAsync(&info, d_info, sizeof(int), cudaMemcpyDeviceToHost, stream));\n",
        "\n",
        "    CHECK(cudaStreamSynchronize(stream));\n",
        "\n",
        "    if (info < 0) {\n",
        "        printf(\"%d-th parameter is wrong\\n\", info);\n",
        "        return 0;\n",
        "    }\n",
        "    else if (info == 0)\n",
        "        printf(\" SVD calculation converges\\n\\n\");\n",
        "    else\n",
        "        printf(\"With info = %d SVD does not converge\\n\", info);\n",
        "\n",
        " //I valori in S sono le singular value decomposition della matrice matrix ordinate in ordine decrescente\n",
        " //I valori in VT sono equivalenti agli autovettori del processo pca standard\n",
        "\n",
        "    CHECK(cudaFree(d_matrix));\n",
        "    CHECK(cudaFree(d_U));\n",
        "    CHECK(cudaFree(d_S));\n",
        "    CHECK(cudaFree(d_VT));\n",
        "    CHECK(cudaFree(d_info));\n",
        "    CHECK(cudaFree(d_work));\n",
        "    CHECK(cudaFree(d_rwork));\n",
        "\n",
        "    cusolverDnDestroy(cusolverH);\n",
        "    CHECK(cudaStreamDestroy(stream));\n",
        "\n",
        "    //Calcolo i Principal Components: T = matrix * VT\n",
        "\n",
        "    principal_components = (float *)malloc (height * height * sizeof(float));\n",
        "\n",
        "    CHECK_CUBLAS(cublasCreate(&handle));\n",
        "    CHECK(cudaMalloc((void **)&d_matrix, size * sizeof(float)));\n",
        "    CHECK(cudaMalloc((void **)&d_VT, size * sizeof(float)));\n",
        "    CHECK(cudaMalloc((void **)&d_C, height * height * sizeof(float)));\n",
        "    CHECK(cudaMemset(d_C, 0, height * height *sizeof(float)));\n",
        "    CHECK_CUBLAS(cublasSetMatrix(height, width, sizeof(float), matrix, height, d_matrix, height));\n",
        "    CHECK_CUBLAS(cublasSetMatrix(height, width, sizeof(float), VT, height, d_VT, height));\n",
        "\n",
        "    alpha = 1.0f;\n",
        "    cudaEventRecord(start);\n",
        "    CHECK_CUBLAS(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, height, width, width, &alpha, d_matrix, height, d_VT, height, &beta, d_C, height));\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\t  cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\t  printf(\" Principal Components calculation took: %.5f (sec)\\n\\n\", milliseconds / 1000.0);\n",
        "    executionTimeSVD += milliseconds;\n",
        "\n",
        "    CHECK_CUBLAS(cublasGetMatrix(height, height, sizeof(float), d_C, height, principal_components, height));\n",
        "\n",
        "    CHECK(cudaFree(d_matrix));\n",
        "    CHECK(cudaFree(d_C));\n",
        "    CHECK(cudaFree(d_VT));\n",
        "\n",
        "    //Valuto quanti Principal components tenere per la riduzione, il parametro r descrive quanta varianza si è interessati a tenere\n",
        "\n",
        "    variance = 0.0;\n",
        "    variance = var(r, width, S);\n",
        "\n",
        "    printf(\" With a parameter of %d elements we were able to keep %f percentage of principal components\\n\", r, variance);\n",
        "    printf(\" PCA technique through SVD calculation took in total %f seconds\\n\\n\", executionTimeSVD / 1000.0);\n",
        "\n",
        "    printf(\" Speedup between classic PCA and SVD is :  %.4f\\n\\n\", executionTime/executionTimeSVD);\n",
        "\n",
        "    printf(\" Calculating PCA through serial algorithm...\\n\");\n",
        "    double timeElapsed = serialPCA(matrix);\n",
        "    printf(\" Serial PCA took %f\\n\", timeElapsed);\n",
        "    printf(\" GPU execution of the PCA algorithm allows a speedup of %.4f when using classic PCA and of %.4f when using SVD\\n\", timeElapsed/executionTime * 1000, timeElapsed/executionTimeSVD * 1000);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "void generateMatrix(int rows, int cols, float **matrix) {\n",
        "\tfloat *temp = (float *) malloc(sizeof(float) * rows * cols);\n",
        "\n",
        "  for (int i = 0; i < cols; i++)\n",
        "    for (int j = 0; j < rows; j++){\n",
        "      temp[i * rows + j] = (float)rand() / (RAND_MAX) * 10;\n",
        "    }\n",
        "\t*matrix = temp;\n",
        "}\n",
        "\n",
        "float* orderInversion(int dim, float *eigenvalues){\n",
        "  float *temp = (float *)malloc (sizeof(float)*dim);\n",
        "\n",
        "  for(int i = 0; i < dim; i++)\n",
        "    temp[i] = eigenvalues[dim - 1 - i];\n",
        "\n",
        "  return temp;\n",
        "}\n",
        "\n",
        "double var(int r, int size, float *v) {\n",
        "\n",
        "  double sum1 = 0.0;\n",
        "  double sum2 = 0.0;\n",
        "\n",
        "  for (int i = 0; i < r; i++)\n",
        "    sum1 += v[i];\n",
        "\n",
        "  for (int i = 0; i < size; i++)\n",
        "    sum2 += v[i];\n",
        "\n",
        "  double variance = sum1/sum2 *100;      \n",
        "\n",
        "  if (variance < 0)\n",
        "    variance = variance + 100;\n",
        "  else if (variance > 100)\n",
        "    variance = 200 - variance;\n",
        "\n",
        "  return variance;\n",
        "\n",
        "}\n",
        "\n",
        "__global__ void meanSubtraction(float* matrix, float *output, int w, int h) {\n",
        "\t\n",
        "\tuint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\tuint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\tfloat sum = 0.0;\n",
        "  float mean = 0.0;\n",
        "  float val = 0.0;\n",
        "\n",
        "\t__shared__ float Ms[BLOCKS][BLOCKS];\n",
        "\n",
        "\tuint numBlocks = (w + BLOCKS - 1) / BLOCKS;\n",
        "\tfor (int m = 0; m < numBlocks; m++) {\n",
        "\t\t\n",
        "\t\tuint c = m * BLOCKS + threadIdx.x;\n",
        "\t\tMs[threadIdx.y][threadIdx.x] = matrix[row * w + c];\n",
        "\n",
        "\t\t__syncthreads();\n",
        "\n",
        "\t\tuint K = BLOCKS;\n",
        "\t\tif (m == numBlocks - 1) K = w - m * BLOCKS; \n",
        "\n",
        "    for (int k = 0; k < K; k++)\n",
        "      for (int x = 0; x < K; x++)\n",
        "        sum += Ms[k][x];\n",
        "\n",
        "      mean = sum / (float)(w*h);\n",
        "\t\t\tval = Ms[threadIdx.y][threadIdx.x] - mean;\n",
        "\n",
        "      __syncthreads();\n",
        "\t}\n",
        "\n",
        "\tif (row < w && col < h)\n",
        "\t\toutput[row * h + col] = val;\n",
        "}\n",
        "\n",
        "\n",
        "double serialPCA (float *matrix){\n",
        "  \n",
        "  double start = seconds();\n",
        "    \n",
        "     int state = C, num_iter = 0, m, k, l; \n",
        "     float p, y, d, r, t;           \n",
        "     double c, s;\n",
        "     float *eigenvectors = (float *)malloc(sizeof(float)*C*C);\n",
        "     initialize_identity(eigenvectors, C); \n",
        "     float *eigenvalues = (float *)malloc(sizeof(float) * C); \n",
        "     int *index = (int *)malloc(sizeof(int) * C);        \n",
        "     bool *changed = (bool *)malloc(sizeof(bool) * C); \n",
        "     float *A_s = (float *)calloc(C * C, sizeof(float));\n",
        "     float *matrix_T = (float *)malloc(sizeof(float) * C * R);\n",
        "     transpose(matrix, R, C, matrix_T);\n",
        "     multiply(matrix_T, C, R, matrix, R, C, A_s);\n",
        "\n",
        "     for (int i = 0; i < C; i++){\n",
        "         index[i] = maxind(A_s, C, i); \n",
        "         eigenvalues[i] = A_s[i * C + i];\n",
        "         changed[i] = true;\n",
        "       }\n",
        "     while (state && num_iter < 10000000){         \n",
        "         m = 0;\n",
        "         for (int i = 1; i < C - 1; i++){          \n",
        "             if (fabs(A_s[i * C + index[i]]) > fabs(A_s[m * C + index[m]]))\n",
        "                 m = i;\n",
        "         }\n",
        "         k = m;\n",
        "         l = index[k];\n",
        "         p = A_s[k * C + l];\n",
        "         y = 0.5 * (eigenvalues[l] - eigenvalues[k]);\n",
        "         d = fabs(y) + sqrt(p * p + y * y);\n",
        "         r = sqrt(p * p + d * d);\n",
        "         c = d / r;\n",
        "         s = p / r;\n",
        "         t = p * p / d;\n",
        "         if (y < 0){\n",
        "             s = -s;\n",
        "             t = -t;\n",
        "         }\n",
        "         A_s[k * C + l] = 0.0;\n",
        "         update(k, -t, eigenvalues, changed, &state);\n",
        "         update(l, t, eigenvalues, changed, &state);\n",
        "\n",
        "         //rotate rows and cols k and l:\n",
        "         for (int i = 0; i < k; i++){\n",
        "             rotate(i, k, i, l, A_s, c, s);\n",
        "         }\n",
        "         for (int i = k + 1; i < l; i++){\n",
        "             rotate(k, i, i, l, A_s, c, s);\n",
        "         }\n",
        "         for (int i = l + 1; i < C; i++){\n",
        "             rotate(k, i, l, i, A_s, c, s);\n",
        "         }\n",
        "         //rotate eigenvectors:\n",
        "         for (int i = 0; i < C; i++){\n",
        "             float e_ik = c * eigenvectors[i * C + k] - s * eigenvectors[i * C + l];\n",
        "             float e_il = s * eigenvectors[i * C + k] + c * eigenvectors[i * C + l];\n",
        "             eigenvectors[i * C + k] = e_ik;\n",
        "             eigenvectors[i * C + l] = e_il;\n",
        "         }\n",
        "         index[k] = maxind(A_s, C, k);\n",
        "         index[l] = maxind(A_s, C, l);\n",
        "         num_iter++;\n",
        "     }\n",
        "     //sort eigenvalues in desc:\n",
        "\n",
        "     int *indices = (int *)malloc(sizeof(int) * C);\n",
        "     for (int i = 0; i < C; i++)\n",
        "         indices[i] = i;\n",
        "     \n",
        "     mergesort(eigenvalues, C, indices, 0, C - 1);\n",
        "\n",
        "     //computing SIGMA:\n",
        "     \n",
        "     float *SIGMA = (float *)calloc(C * R, sizeof(float));\n",
        "     float sum=0.0;\n",
        "     for (int i = 0; i < C; i++){\n",
        "         SIGMA[i] = sqrt(eigenvalues[i]);\n",
        "         sum+=eigenvalues[i];     \n",
        "     }\n",
        "       \n",
        "    //eigenvectors matrix (U for matrix_T*matrix):\n",
        "   \n",
        "     float *u_s = (float *) malloc(sizeof(float)*C*C);\n",
        "     for (int row = 0; row < C; row++){\n",
        "         for (int col = 0; col < C; col++)\n",
        "             u_s[row * C + col] = eigenvectors[row * C + indices[col]];  \n",
        "     }\n",
        "     //compute V_T:\n",
        "     float *V_T_s = (float *)calloc(R*R, sizeof(float));\n",
        "     compute_V(SIGMA, matrix_T, &u_s, &V_T_s);     \n",
        "\n",
        "     //compute serial PCA:\n",
        "      int K_s=0;\n",
        "      float retention_s = 0.0;\n",
        "      int count_s = 0;\n",
        "      while((retention_s < P) && (count_s < C)){\n",
        "          retention_s+=(SIGMA[count_s]*SIGMA[count_s]/sum)*100; \n",
        "          K_s++;\n",
        "          count_s++;\n",
        "      }\n",
        "      \n",
        "      float *W_s = (float *)malloc(sizeof(float)*C*K_s);\n",
        "      float *principal_components = (float *)malloc(sizeof(float)*R*K_s);\n",
        "      for (int r=0; r<C; r++)\n",
        "      {\n",
        "          for (int c=0; c<K_s; c++)\n",
        "          {\n",
        "              W_s[r*K_s+c] = u_s[r*C+c];\n",
        "          }\n",
        "      }\n",
        "     multiply(matrix, R, C, W_s, C, K_s, principal_components);\n",
        "    \n",
        "     return(seconds() - start);  \n",
        "}\n",
        "\n",
        "void initialize_identity(float *I, int size){\n",
        "    memset(I, 0, sizeof(float)*size*size);\n",
        "    for (int i = 0; i < size; i++)\n",
        "        I[i * size + i] = 1.0;\n",
        "}\n",
        "\n",
        "void transpose(float *M, int m, int n, float *M_T){\n",
        "    int i, j, index_;\n",
        "    for (j=0; j<n; j++){\n",
        "        index_ = j*m;\n",
        "        for (i=0; i<m; i++)\n",
        "            M_T[index_+i] = M[i*n+j];\n",
        "    }\n",
        "}\n",
        "\n",
        "void multiply(float *M_1, int m1, int n1, float *M_2, int m2, int n2, float *result){\n",
        "    \n",
        "    float sum = 0.0;\n",
        "   \n",
        "    float *M_2_T = (float *)malloc(sizeof(float) * n2 * m2);\n",
        "    transpose(M_2, m2, n2, M_2_T);\n",
        "    int i, j, k, temp1, temp2;\n",
        "    for (i = 0; i < m1; i++){\n",
        "        temp1 = i * n1;\n",
        "        for (j = 0; j < n2; j++){\n",
        "            sum = 0.0;\n",
        "            temp2 = j * m2;\n",
        "            for (k = 0; k < n1; k++)\n",
        "                sum += M_1[temp1 + k] * M_2_T[temp2 + k];\n",
        "            result[i * n2 + j] = sum;\n",
        "        }\n",
        "    }\n",
        "    free(M_2_T);\n",
        "}\n",
        "\n",
        "float maxind(float *A, int size, int k){\n",
        "    int m = k + 1;\n",
        "    for (int i = k + 2; i < size; i++){\n",
        "        if (fabs(A[k * size + i]) > fabs(A[k * size + m]))\n",
        "            m = i;\n",
        "    }\n",
        "    return m;\n",
        "}\n",
        "\n",
        "void update(int k, float t, float *e, bool *changed, int *state){\n",
        "    float y = e[k];\n",
        "    e[k] = y + t;\n",
        "    if (changed[k] && (y == e[k])){\n",
        "        changed[k] = false;\n",
        "        (*state)--;\n",
        "    }\n",
        "    else if (!changed[k] && (y != e[k])){\n",
        "        changed[k] = true;\n",
        "        (*state)++;\n",
        "    }\n",
        "}\n",
        "\n",
        "void rotate(int k, int l, int i, int j, float *A, double c, double s){\n",
        "    float k_l = c * A[k * C + l] - s * A[i * C + j];\n",
        "    float i_j = s * A[k * C + l] + c * A[i * C + j];\n",
        "    A[k * C + l] = k_l;\n",
        "    A[i * C + j] = i_j;\n",
        "}\n",
        "\n",
        "void compute_V(float *SIGMA, float *matrix_T, float **U, float **V_T){\n",
        "    \n",
        "    float *INV_SIGMA = (float *)calloc(R * C, sizeof(float)); \n",
        "    for (int i = 0; i < C; i++)\n",
        "        INV_SIGMA[i * C + i] = 1.0 / (SIGMA[i]);\n",
        "  \n",
        "    float *U_T = (float *)malloc(sizeof(float) * C * C);\n",
        "    transpose(*U, C, C, U_T);\n",
        "    \n",
        "    float *product = (float *)malloc(sizeof(float) * R * C);\n",
        "    multiply(INV_SIGMA, R, C, U_T, C, C, product);\n",
        "   \n",
        "    multiply(product, R, C, matrix_T, C, R, *V_T);\n",
        "    free(INV_SIGMA);\n",
        "    free(U_T);\n",
        "    free(product);\n",
        "}\n",
        "\n",
        "void merge(float *e, int *indices_e, int left_index, int mid, int right_index){\n",
        "    int i = left_index, j = mid + 1, k = 0;\n",
        "    float *sorted = (float *)malloc(sizeof(float) * (right_index - left_index + 1));\n",
        "    int *sorted_indices = (int *)malloc(sizeof(int) * (right_index - left_index + 1));\n",
        "    \n",
        "    while (i <= mid && j <= right_index){\n",
        "        if (fabs(e[i]) >= fabs(e[j])){\n",
        "            sorted_indices[k] = indices_e[i];\n",
        "            sorted[k++] = e[i++];\n",
        "        }\n",
        "        else{\n",
        "            sorted_indices[k] = indices_e[j];\n",
        "            sorted[k++] = e[j++];\n",
        "        }\n",
        "    }\n",
        "    while (i <= mid){\n",
        "        sorted_indices[k] = indices_e[i];\n",
        "        sorted[k++] = e[i++];\n",
        "    }\n",
        "    while (j <= right_index){\n",
        "        sorted_indices[k] = indices_e[j];\n",
        "        sorted[k++] = e[j++];\n",
        "    }\n",
        "    \n",
        "    memcpy(e + left_index, sorted, sizeof(float)*(right_index-left_index+1));\n",
        "    memcpy(indices_e + left_index, sorted_indices, sizeof(int)*(right_index-left_index+1));\n",
        "    free(sorted);\n",
        "    free(sorted_indices);\n",
        "}\n",
        "\n",
        "void mergesort(float *e, int e_len, int *indices_e, int left_index, int right_index){\n",
        "    if (left_index < right_index){\n",
        "        int mid = (left_index + right_index) / 2;\n",
        "        mergesort(e, e_len, indices_e, left_index, mid);\n",
        "        mergesort(e, e_len, indices_e, mid + 1, right_index);\n",
        "        merge(e, indices_e, left_index, mid, right_index);\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taXCfMNmZsiH",
        "outputId": "906e2bba-78a5-4a7b-a6d8-33900af6918c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/pca.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 src/pca.cu -o pca -lcublas -lcusolver\n",
        "!./pca"
      ],
      "metadata": {
        "id": "6MhHC20ngnXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il codice è stato testato con diversi valori per righe e colonne di matrice.\n",
        "\n",
        "Con 20 righe e 10 colonne il codice sequenziale è il più performante di circa 5 volte per PCA con autovalori e 10 volte per SVD.\n",
        "\n",
        "All'aumentare dei valori di righe e colonne il codice sequenziale diventa sempre più lento rispetto a quello parallelo, è stato testato con 2000 righe e 300 colonne un tempo di esecuzione di circa 2 minuti mentre PCA attraverso calcolo di autovalori o SVD si attestano intorno ai 0.2 e 0.03 secondi con un visibile vantaggio della tecnica SVD.\n",
        "\n",
        "Per valori intermedi (è stato testato con 200 righe e 100 colonne) sarà invece la tecnica PCA con autovettori ad essere la più veloce.\n",
        "\n",
        "Per concludere all'aumentare dei valori di righe e colonne con conseguente aumento di rilevazioni (di numeri) del sistema la tecnica parallela con SVD risulta la più performante."
      ],
      "metadata": {
        "id": "r7Gdjs9GkB14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile ./pca\n",
        "!nsys stats ../content/report1.qdrep"
      ],
      "metadata": {
        "id": "osrAKinqyF1X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}